{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def init():\n",
    "    #CHANGE THIS TO YOUR TRAIN FOLDER\n",
    "image_folder = 'C:/Study/Semester2/Machine Learning/ML_images/Training' \n",
    "#image_folder = 'C:/Users/danie/Desktop/Training_ML'\n",
    "\n",
    "TOO_SMALL = 200000\n",
    "SMALL = 500000\n",
    "MEDIUM = 1200000\n",
    "LARGE = 2000000\n",
    "    \n",
    "X_train = []\n",
    "Y_train = []\n",
    "counter = 1\n",
    "    \n",
    "img_height = 1888 #Usually 128. this might change depending on Annaya & Danielle's input \n",
    "img_width = 1888 #Usually 128. this might change depending on Annaya & Danielle's input \n",
    "epochs = 1 #Start with 1, and increase to 10, 100, 500, 1000 and 3000. 11 is the recommended number of runs through the training dataset. We will probably have to tune this. \n",
    "#total_training = total number of training data len()\n",
    "batch_size = 1 #test different ones from 1 to 2, 5, 10, etc\n",
    "total_images = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "6ba26cc8e50f49b5b9b751f88421f6cc",
    "deepnote_cell_height": 459,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10265,
    "execution_start": 1652547014086,
    "source_hash": "25a5d69b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, UpSampling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage import data, io,color\n",
    "from skimage.io import imread_collection #loads a collection of images\n",
    "from skimage.io import imread, imshow\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "5647a315e91e408b8dcdd46d33814f9e",
    "deepnote_cell_height": 477,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1652547146694,
    "source_hash": "8fbce57",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define sizes of images based on their resolution\n",
    "#Image shape and resolution\n",
    "\n",
    "#TODO make the IMG sizes make sense\n",
    "\n",
    "\n",
    "def shape(img):\n",
    "    size = 0\n",
    "    resolution = img.shape[0] * img.shape[1]\n",
    "    if resolution <= TOO_SMALL:\n",
    "        size = 0\n",
    "    elif resolution <= SMALL:\n",
    "        size = 1\n",
    "    elif resolution <= MEDIUM:\n",
    "        size = 2\n",
    "    elif resolution <= LARGE:\n",
    "        size = 3\n",
    "    else:\n",
    "        size = 6\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(image_folder):\n",
    "    images = Path(image_folder).glob('*.jpg')\n",
    "\n",
    "    list_of_files = []\n",
    "    for image in images:\n",
    "        list_of_files.append(str(image))\n",
    "\n",
    "    df = pd. DataFrame(list_of_files, columns = [\"image_path\"])\n",
    "    df_length = len(df.index)\n",
    "    print(\"processing\", df_length, \"photos for training\")\n",
    "    \n",
    "    height= []\n",
    "    width = []\n",
    "    resolution = []\n",
    "    \n",
    "    for image_path in df['image_path']:\n",
    "        img = cv2.imread(str(image_path))\n",
    "        height.append(img.shape[0])\n",
    "        width.append(img.shape[1])\n",
    "        \n",
    "    df[\"height\"] = height\n",
    "    df[\"width\"] = width\n",
    "    df[\"resolution\"] = df[\"height\"] * df[\"width\"]\n",
    "    \n",
    "    df = df[df.resolution < LARGE]\n",
    "    \n",
    "    ll = df_length - len(df.index)\n",
    "    print(\"dropped \", ll, \"photos because they were too large to process\")\n",
    "    \n",
    "    df = df[df.resolution > TOO_SMALL]\n",
    "    \n",
    "    ss = df_length - len(df.index) - ll\n",
    "    print(\"dropped \", ss, \"photos because they were too small to process\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "284e594fca804177bb7508f8e816bfa2",
    "deepnote_cell_height": 387,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652547146695,
    "source_hash": "4c92008b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PCA rgb then pass to prep_img as an rgb image\n",
    "\n",
    "def pca_rgb(imgRGB, size): #imgLAB imgRGB is fresh images from col\n",
    "    b,g,r = cv2.split(imgRGB)\n",
    "    list_channels = [b,g,r]\n",
    "    \n",
    "    #check size first\n",
    "    if size ==1:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 100)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        rgb_compressed = cv2.merge(inverted_img)    \n",
    "\n",
    "    elif size == 2:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 300)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        rgb_compressed = cv2.merge(inverted_img)\n",
    "    else:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 500)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        rgb_compressed = cv2.merge(inverted_img)\n",
    "\n",
    "    return rgb_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image padding\n",
    "\n",
    "def pad(image):\n",
    "    \n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "\n",
    "    WHITE = [255,255,255]\n",
    "    MAX = 1888\n",
    "    \n",
    "    if height % 2 == 0:\n",
    "        top = int((MAX - height)/ 2)\n",
    "        bottom = int((MAX - height)/ 2)\n",
    "    else: \n",
    "        top = int((MAX - (height-1))/ 2)\n",
    "        bottom = int(((MAX - (height-1))/ 2)-1)\n",
    "\n",
    "    if width % 2 == 0:\n",
    "        left = int((MAX - width)/ 2)\n",
    "        right = int((MAX - width)/ 2)\n",
    "    else: \n",
    "        left = int((MAX - (width-1))/ 2)\n",
    "        right = int(((MAX - (width-1))/2)-1)\n",
    "    \n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right,\n",
    "    cv2.BORDER_CONSTANT,value=WHITE)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "6be9a9aec0ee44dab9bb55d664de627f",
    "deepnote_cell_height": 693,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652547146783,
    "source_hash": "c4f5421",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function to preprocess image: by check image category, compress, separate color channels, and pad\n",
    "# gives out padded images in 2 groups,which are padded Lambda channel (Black&white) and AB channel(colors)\n",
    "\n",
    "#We can decide to take file path or im collection  or list\n",
    "#note: we are most familiar with controlling list\n",
    "n = 1\n",
    "\n",
    "def prep_img(img_path):\n",
    "    imgRGB = cv2.imread(img_path)\n",
    "\n",
    "    #check the image category by 'shape' function\n",
    "    image_size_category = shape(imgRGB)\n",
    "    #print(\"image size determined!\")\n",
    "\n",
    "    if image_size_category <1:\n",
    "        raise Exception(\"Your image is too small to be used\")\n",
    "    elif image_size_category >4:\n",
    "        raise Exception(\"Your image is too big to be used\")\n",
    "\n",
    "    #print(\"image PCA started!\")\n",
    "    image_pca = pca_rgb(imgRGB, image_size_category) \n",
    " \n",
    "    #image_pca.astype(\"float32\")/ 255 \n",
    "    image_pca = np.float32(1.0/255*image_pca)\n",
    "    imgLAB = cv2.cvtColor(1.0/255*image_pca, cv2.COLOR_RGB2Lab)\n",
    "    \n",
    "    print(\"image converted to LAB\")\n",
    "    padded_image = pad(imgLAB)\n",
    "    \n",
    "    #separate L and AB color channels\n",
    "    Y = padded_image[:, :,1:]\n",
    "    X = padded_image[:, :, 0]\n",
    "    \n",
    "    Y = Y / 128\n",
    "    #this is in sample code and idk why\n",
    "    \n",
    "    X = X.reshape(1, 1888, 1888, 1)\n",
    "    Y = Y.reshape(1, 1888, 1888, 2)\n",
    "    \n",
    "    print(\"image processed\")\n",
    "\n",
    "    return Y, X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataframe(df):\n",
    "    for index, row in df.iterrows():\n",
    "        Y, X = prep_img(row[\"image_path\"])\n",
    "        Y_train.append(Y)\n",
    "        X_train.append(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the code below to process ALL photos and feed them to the model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0 photos for training\n",
      "dropped  0 photos because they were too large to process\n",
      "dropped  0 photos because they were too small to process\n"
     ]
    }
   ],
   "source": [
    "df = create_dataframe(image_folder)\n",
    "prep_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(): \n",
    "    model1 = Sequential([\n",
    "        #encoder\n",
    "        Conv2D(128, 2, strides = 2, padding = \"same\", activation = \"relu\", input_shape= (1888, 1888, 1)), #testing shape changes\n",
    "        Conv2D(256, 2, strides = 2, padding = \"same\", activation = \"relu\"), \n",
    "        Conv2D(256, 2, strides = 2, padding = \"same\", activation = \"relu\"), \n",
    "\n",
    "        #decoder\n",
    "        Conv2D(256, 2, padding = \"same\", activation = \"relu\"), \n",
    "        UpSampling2D(2),\n",
    "        Conv2D(128, 2, padding = \"same\", activation = \"relu\"), \n",
    "        UpSampling2D(2), \n",
    "        Conv2D(2, 2, padding = \"same\", activation = \"tanh\"),\n",
    "        UpSampling2D(2),\n",
    "    ])\n",
    "    model1.compile(optimizer =\"adam\", loss = \"mse\", metrics = [\"accuracy\"])\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = mode()\n",
    "history = model1.fit(train_data, batch_size = batch_size, epochs = epochs, verbose = 2)  # add validation_split = 0.2\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(history, filename)\n",
    " \n",
    "\n",
    "#loaded_model = joblib.load(filename)\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)\n"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "db6bc5cd-12ac-4fa4-b95f-4604d6441ef4",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
