{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "cell_id": "38d8d17f-4618-4e8f-9c2d-99d69fa70f05",
    "deepnote_cell_height": 387,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652989328388,
    "source_hash": "d3d0c034",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#single_image = r\"C:\\Users\\danie\\Desktop\\tiny rainbow.jpg\"\n",
    "image_folder = r\"C:\\Users\\midyr\\Downloads\\colorwheels\"\n",
    "image_pathway = r\"C:\\Users\\midyr\\Downloads\\traffic4.jpg\"\n",
    "\n",
    "TOO_SMALL = 20\n",
    "SMALL = 500000\n",
    "MEDIUM = 1200000\n",
    "LARGE = 2100000\n",
    "    \n",
    "X_train = []\n",
    "Y_train = []\n",
    "counter = 1\n",
    "    \n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "total_images = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "cell_id": "b8efae5f194149089dedc9b820f4a58e",
    "deepnote_cell_height": 441,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9072,
    "execution_start": 1652989328389,
    "source_hash": "91d9564f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, UpSampling2D, InputLayer, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage import data, io, color\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "cell_id": "32adad948e9a4979861366f903471478",
    "deepnote_cell_height": 315,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9,
    "execution_start": 1652989337467,
    "source_hash": "e98c48e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shape(img):\n",
    "    size = 0\n",
    "    resolution = img.shape[0] * img.shape[1]\n",
    "    if resolution <= TOO_SMALL:\n",
    "        size = 0\n",
    "    elif resolution <= SMALL:\n",
    "        size = 1\n",
    "    elif resolution <= MEDIUM:\n",
    "        size = 2\n",
    "    elif resolution <= LARGE:\n",
    "        size = 3\n",
    "    else:\n",
    "        size = 6\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "cell_id": "2680a5ba2be841bfbdb63a2c8a3cb25f",
    "deepnote_cell_height": 729,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1652989337490,
    "source_hash": "d7aa9571",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataframe(image_folder):\n",
    "    images = Path(image_folder).glob('*.jpg') #.png\n",
    "\n",
    "    list_of_files = []\n",
    "    for image in images:\n",
    "        list_of_files.append(str(image))\n",
    "\n",
    "    df = pd. DataFrame(list_of_files, columns = [\"image_path\"])\n",
    "    df_length = len(df.index)\n",
    "    \n",
    "    print(\"processing\", df_length, \"photos for training\")\n",
    "    \n",
    "    height_list= []\n",
    "    width_list = []\n",
    "    resolution = []\n",
    "    \n",
    "    for image_path in df['image_path']:\n",
    "        img = cv2.imread(str(image_path))\n",
    "        height_list.append(img.shape[0])\n",
    "        width_list.append(img.shape[1])\n",
    "        \n",
    "    df[\"height\"] = height_list\n",
    "    df[\"width\"] = width_list\n",
    "    df[\"resolution\"] = df[\"height\"] * df[\"width\"]\n",
    "    \n",
    "    df = df[df.resolution < LARGE]\n",
    "    \n",
    "    ll = df_length - len(df.index)\n",
    "    print(\"dropped \", ll, \"photos because they were too large to process\")\n",
    "    \n",
    "    df = df[df.resolution > TOO_SMALL]\n",
    "    \n",
    "    ss = df_length - len(df.index) - ll\n",
    "    print(\"dropped \", ss, \"photos because they were too small to process\")\n",
    "    df.head()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "cell_id": "f9d01eb4c10c4a4185fb7623d2498aee",
    "deepnote_cell_height": 783,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652989337551,
    "source_hash": "a125419d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pca_rgb(imgBGR, size): #imgRGB, size\n",
    "    b,g,r = cv2.split(imgBGR)\n",
    "    list_channels = [b,g,r]\n",
    "    \n",
    "    #check size first\n",
    "    if size ==1:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 100)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        RGB_compressed = cv2.merge(inverted_img)    \n",
    "\n",
    "    elif size == 2:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 300)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        RGB_compressed = cv2.merge(inverted_img)\n",
    "    else:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 500)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        RGB_compressed = cv2.merge(inverted_img)\n",
    "        \n",
    "    RGB_compressed = np.float32(RGB_compressed)\n",
    "    print(\"PCA SHAPE\",np.min(RGB_compressed),np.max(RGB_compressed))    \n",
    "    return RGB_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "cell_id": "abf7b76ddae14704a2acd6bce2c5d886",
    "deepnote_cell_height": 567,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652989337552,
    "source_hash": "a86c0dee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rgb_pad(image):\n",
    "    \n",
    "    img_height = image.shape[0]\n",
    "    print(height)\n",
    "    img_width = image.shape[1]\n",
    "    print(width)\n",
    "    \n",
    "    WHITE = [255,255,255] #[255,255,255]  1, 1, 1\n",
    "    \n",
    "    if img_height % 2 == 0:\n",
    "        top = int((height - img_height)/ 2)\n",
    "        bottom = int((height - img_height)/ 2)\n",
    "    else: \n",
    "        top = int((height - (img_height-1))/ 2)\n",
    "        bottom = int(((height - (img_height-1))/ 2)-1)\n",
    "\n",
    "    if img_width % 2 == 0:\n",
    "        left = int((width - img_width)/ 2)\n",
    "        right = int((width - img_width)/ 2)\n",
    "    else: \n",
    "        left = int((width - (img_width-1))/ 2)\n",
    "        right = int(((width - (img_width-1))/2)-1)\n",
    "    \n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT,value=WHITE)\n",
    "    print(image.shape)\n",
    "    print(np.min(image),np.max(image))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "cell_id": "bc2d9f2a8c394d1a8eaf03cc82941994",
    "deepnote_cell_height": 927,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652989337596,
    "source_hash": "71afb187",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_img(img_path):\n",
    "    imgRGB = img_to_array(load_img(img_path))\n",
    "    imgRGB = np.array(imgRGB, dtype=float)\n",
    "    print('Padded:', np.min(imgRGB), np.max(imgRGB))\n",
    "\n",
    "\n",
    "\n",
    "    image_size_category = shape(imgRGB) \n",
    "    if image_size_category <1:\n",
    "        raise Exception(\"Your image is too small to be used\")\n",
    "    elif image_size_category >4:\n",
    "        raise Exception(\"Your image is too big to be used\")\n",
    "\n",
    "    if PCA_YN == \"yes\":\n",
    "        RGB_compressed = pca_rgb(imgRGB, image_size_category)\n",
    "        RGB_compressed = np.clip(RGB_compressed,0,1) * 255\n",
    "        print(\"PCA ran\")\n",
    "    else:\n",
    "        RGB_compressed = imgRGB\n",
    "        print(\"PCA did not run\")\n",
    "\n",
    "\n",
    "    \n",
    "    padded_image = rgb_pad(RGB_compressed)\n",
    "\n",
    "    print('Padded:', np.min(padded_image), np.max(padded_image))\n",
    "    X_LAB = color.rgb2lab(1.0/255*padded_image)[:,:,0] #1.0/255*padded_image\n",
    "    print('X_LAB:', np.min(X_LAB), np.max(X_LAB))\n",
    "    Y_LAB1 = color.rgb2lab(1.0/255*padded_image)[:,:,1:] #1.0/255*padded_image\n",
    "    print('Y_LAB_org:', np.min(Y_LAB1), np.max(Y_LAB1))\n",
    "    #Y_LAB = Y_LAB1 / 128\n",
    "    Y_LAB = (Y_LAB1 + 128)/ 256\n",
    "    print('Y_LAB:', np.min(Y_LAB), np.max(Y_LAB))\n",
    "\n",
    "\n",
    "    print(X_LAB.shape)\n",
    "    X = X_LAB.reshape(1, height, width, 1) #comment out when show predicted\n",
    "    Y = Y_LAB.reshape(1, height, width, 2) #comment out when show predicted\n",
    "    \n",
    "    print(\"image processed\")\n",
    "\n",
    "    return Y, X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "cell_id": "08ce9bd97ede45efb37200aa51c02ffd",
    "deepnote_cell_height": 153,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1652989337596,
    "source_hash": "12af0869",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_dataframe(df):\n",
    "    for index, row in df.iterrows():\n",
    "        Y, X = prep_img(row[\"image_path\"])\n",
    "        Y_train.append(Y)\n",
    "        X_train.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "cell_id": "a43e064ffe25479d8fef8cc5a0dbab72",
    "deepnote_cell_height": 726,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     424.390625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1606,
    "execution_start": 1652989337597,
    "source_hash": "26e413f6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 4 photos for training\n",
      "dropped  0 photos because they were too large to process\n",
      "dropped  0 photos because they were too small to process\n",
      "Padded: 0.0 255.0\n",
      "PCA did not run\n",
      "256\n",
      "256\n",
      "(256, 256, 3)\n",
      "0.0 255.0\n",
      "Padded: 0.0 255.0\n",
      "X_LAB: 0.17702971378012577 100.0\n",
      "Y_LAB_org: -71.17268893328688 77.75729265802376\n",
      "Y_LAB: 0.22198168385434813 0.8037394244454052\n",
      "(256, 256)\n",
      "image processed\n",
      "Padded: 0.0 255.0\n",
      "PCA did not run\n",
      "256\n",
      "256\n",
      "(256, 256, 3)\n",
      "0.0 255.0\n",
      "Padded: 0.0 255.0\n",
      "X_LAB: 0.039573654069084085 100.0\n",
      "Y_LAB_org: -90.63866915032773 83.20574458892847\n",
      "Y_LAB: 0.1459426986315323 0.8250224398005018\n",
      "(256, 256)\n",
      "image processed\n",
      "Padded: 0.0 255.0\n",
      "PCA did not run\n",
      "256\n",
      "256\n",
      "(256, 256, 3)\n",
      "0.0 255.0\n",
      "Padded: 0.0 255.0\n",
      "X_LAB: 0.8013688253736433 100.0\n",
      "Y_LAB_org: -48.95291303408261 90.26595316207187\n",
      "Y_LAB: 0.3087776834606148 0.8526013795393432\n",
      "(256, 256)\n",
      "image processed\n",
      "Padded: 0.0 255.0\n",
      "PCA did not run\n",
      "256\n",
      "256\n",
      "(256, 256, 3)\n",
      "0.0 255.0\n",
      "Padded: 0.0 255.0\n",
      "X_LAB: 0.4324681123001284 99.45842141301871\n",
      "Y_LAB_org: -37.06652323505899 65.92459281365637\n",
      "Y_LAB: 0.35520889361305086 0.7575179406783452\n",
      "(256, 256)\n",
      "image processed\n"
     ]
    }
   ],
   "source": [
    "PCA_YN = \"no\"\n",
    "\n",
    "#prep_image(single_image)\n",
    "\n",
    "df = create_dataframe(image_folder)\n",
    "prep_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "LEARNING_RATE = 0.001 #0.001\n",
    "EPOCHS = 500 #500\n",
    "KERNEL_SIZE = 2 \n",
    "NUM_CLASSES = 64 #64\n",
    "#IMAGE_WIDTH = 128 #128\n",
    "#IMAGE_HEIGHT = 128 #128\n",
    "#SIGMA = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode():\n",
    "   \n",
    "    model = tf.keras.Sequential([\n",
    "    Conv2D(64, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(64, kernel_size=KERNEL_SIZE, activation='relu', padding='same', strides=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(128, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(128, kernel_size=KERNEL_SIZE, activation='relu', padding='same', strides=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(256, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(256, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(256, kernel_size=KERNEL_SIZE, activation='relu', padding='same', strides=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same', dilation_rate=2),\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same', dilation_rate=2),\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same', dilation_rate=2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same', dilation_rate=2),\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same', dilation_rate=2),\n",
    "    Conv2D(512, kernel_size=KERNEL_SIZE, activation='relu', padding='same', dilation_rate=2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(256, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(256, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(256, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    UpSampling2D(size=(2, 2)),\n",
    "    Conv2D(128, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(128, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    Conv2D(128, kernel_size=KERNEL_SIZE, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(64, kernel_size=KERNEL_SIZE, padding='same'),\n",
    "    Conv2D(64, kernel_size=KERNEL_SIZE, padding='same'),\n",
    "    Conv2D(64, kernel_size=KERNEL_SIZE, padding='same'),\n",
    "    BatchNormalization(),\n",
    "        \n",
    "    UpSampling2D(size=(2, 2)),\n",
    "    Conv2D(16, kernel_size=KERNEL_SIZE, padding='same'),    \n",
    "    Conv2D(16, kernel_size=KERNEL_SIZE, padding='same'),\n",
    "    Conv2D(16, kernel_size=KERNEL_SIZE, padding='same'),    \n",
    "    BatchNormalization(), \n",
    "        \n",
    "    Conv2D(2, kernel_size=KERNEL_SIZE, padding='same'),\n",
    "    Conv2D(2, kernel_size=KERNEL_SIZE, padding='same'),\n",
    "    Conv2D(2, kernel_size=KERNEL_SIZE, padding='same'),\n",
    "    BatchNormalization(),\n",
    "        \n",
    "    UpSampling2D(size=(2, 2))])\n",
    "    \n",
    "    model.compile(optimizer =\"rmsprop\", loss = \"mae\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model \n",
    "\n",
    "#?? Acitivation function and Loss function #CategoricalCrossentropy\n",
    "#Conv2D(NUM_CLASSES, kernel_size=1, padding='same', activation='softmax'), UpSampling2D(size=(4, 4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_341 (Conv2D)         (1, 256, 256, 64)         320       \n",
      "                                                                 \n",
      " conv2d_342 (Conv2D)         (1, 128, 128, 64)         16448     \n",
      "                                                                 \n",
      " batch_normalization_121 (Ba  (1, 128, 128, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_343 (Conv2D)         (1, 128, 128, 128)        32896     \n",
      "                                                                 \n",
      " conv2d_344 (Conv2D)         (1, 64, 64, 128)          65664     \n",
      "                                                                 \n",
      " batch_normalization_122 (Ba  (1, 64, 64, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_345 (Conv2D)         (1, 64, 64, 256)          131328    \n",
      "                                                                 \n",
      " conv2d_346 (Conv2D)         (1, 64, 64, 256)          262400    \n",
      "                                                                 \n",
      " conv2d_347 (Conv2D)         (1, 32, 32, 256)          262400    \n",
      "                                                                 \n",
      " batch_normalization_123 (Ba  (1, 32, 32, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_348 (Conv2D)         (1, 32, 32, 512)          524800    \n",
      "                                                                 \n",
      " conv2d_349 (Conv2D)         (1, 32, 32, 512)          1049088   \n",
      "                                                                 \n",
      " conv2d_350 (Conv2D)         (1, 32, 32, 512)          1049088   \n",
      "                                                                 \n",
      " batch_normalization_124 (Ba  (1, 32, 32, 512)         2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_351 (Conv2D)         (1, 32, 32, 512)          1049088   \n",
      "                                                                 \n",
      " conv2d_352 (Conv2D)         (1, 32, 32, 512)          1049088   \n",
      "                                                                 \n",
      " conv2d_353 (Conv2D)         (1, 32, 32, 512)          1049088   \n",
      "                                                                 \n",
      " batch_normalization_125 (Ba  (1, 32, 32, 512)         2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_354 (Conv2D)         (1, 32, 32, 512)          1049088   \n",
      "                                                                 \n",
      " conv2d_355 (Conv2D)         (1, 32, 32, 512)          1049088   \n",
      "                                                                 \n",
      " conv2d_356 (Conv2D)         (1, 32, 32, 512)          1049088   \n",
      "                                                                 \n",
      " batch_normalization_126 (Ba  (1, 32, 32, 512)         2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_357 (Conv2D)         (1, 32, 32, 256)          524544    \n",
      "                                                                 \n",
      " conv2d_358 (Conv2D)         (1, 32, 32, 256)          262400    \n",
      "                                                                 \n",
      " conv2d_359 (Conv2D)         (1, 32, 32, 256)          262400    \n",
      "                                                                 \n",
      " batch_normalization_127 (Ba  (1, 32, 32, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " up_sampling2d_33 (UpSamplin  (1, 64, 64, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_360 (Conv2D)         (1, 64, 64, 128)          131200    \n",
      "                                                                 \n",
      " conv2d_361 (Conv2D)         (1, 64, 64, 128)          65664     \n",
      "                                                                 \n",
      " conv2d_362 (Conv2D)         (1, 64, 64, 128)          65664     \n",
      "                                                                 \n",
      " batch_normalization_128 (Ba  (1, 64, 64, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_363 (Conv2D)         (1, 64, 64, 64)           32832     \n",
      "                                                                 \n",
      " conv2d_364 (Conv2D)         (1, 64, 64, 64)           16448     \n",
      "                                                                 \n",
      " conv2d_365 (Conv2D)         (1, 64, 64, 64)           16448     \n",
      "                                                                 \n",
      " batch_normalization_129 (Ba  (1, 64, 64, 64)          256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " up_sampling2d_34 (UpSamplin  (1, 128, 128, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_366 (Conv2D)         (1, 128, 128, 16)         4112      \n",
      "                                                                 \n",
      " conv2d_367 (Conv2D)         (1, 128, 128, 16)         1040      \n",
      "                                                                 \n",
      " conv2d_368 (Conv2D)         (1, 128, 128, 16)         1040      \n",
      "                                                                 \n",
      " batch_normalization_130 (Ba  (1, 128, 128, 16)        64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_369 (Conv2D)         (1, 128, 128, 2)          130       \n",
      "                                                                 \n",
      " conv2d_370 (Conv2D)         (1, 128, 128, 2)          18        \n",
      "                                                                 \n",
      " conv2d_371 (Conv2D)         (1, 128, 128, 2)          18        \n",
      "                                                                 \n",
      " batch_normalization_131 (Ba  (1, 128, 128, 2)         8         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " up_sampling2d_35 (UpSamplin  (1, 256, 256, 2)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,082,718\n",
      "Trainable params: 11,077,818\n",
      "Non-trainable params: 4,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build((1,256,256,1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "cell_id": "b408bbd7ead74262b6bf4f2fc3691d43",
    "deepnote_cell_height": 81,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1963,
    "execution_start": 1652989341329,
    "source_hash": "e38a0e91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "cell_id": "9ded4b9e929745549791c20ccebfc72e",
    "deepnote_cell_height": 118.1875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1652989343302,
    "source_hash": "234d3646",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "951fd94af6fa4825852a26060bfbdaca",
    "deepnote_cell_height": 162,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 726,
    "execution_start": 1652989343318,
    "owner_user_id": "adc13720-ac36-49d2-a3fd-344e170627ad",
    "source_hash": "eabac380",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 - 4s - loss: 0.8287 - accuracy: 0.4563 - 4s/epoch - 1s/step\n",
      "Epoch 2/500\n",
      "4/4 - 2s - loss: 0.6996 - accuracy: 0.4275 - 2s/epoch - 508ms/step\n",
      "Epoch 3/500\n",
      "4/4 - 2s - loss: 0.6793 - accuracy: 0.4412 - 2s/epoch - 517ms/step\n",
      "Epoch 4/500\n",
      "4/4 - 2s - loss: 0.6250 - accuracy: 0.4104 - 2s/epoch - 554ms/step\n",
      "Epoch 5/500\n",
      "4/4 - 2s - loss: 0.6028 - accuracy: 0.4415 - 2s/epoch - 505ms/step\n",
      "Epoch 6/500\n",
      "4/4 - 2s - loss: 0.5446 - accuracy: 0.4890 - 2s/epoch - 487ms/step\n",
      "Epoch 7/500\n",
      "4/4 - 2s - loss: 0.4992 - accuracy: 0.5019 - 2s/epoch - 491ms/step\n",
      "Epoch 8/500\n",
      "4/4 - 2s - loss: 0.4858 - accuracy: 0.5076 - 2s/epoch - 487ms/step\n",
      "Epoch 9/500\n",
      "4/4 - 2s - loss: 0.4812 - accuracy: 0.5065 - 2s/epoch - 493ms/step\n",
      "Epoch 10/500\n",
      "4/4 - 2s - loss: 0.4769 - accuracy: 0.5025 - 2s/epoch - 493ms/step\n",
      "Epoch 11/500\n",
      "4/4 - 2s - loss: 0.4728 - accuracy: 0.4987 - 2s/epoch - 486ms/step\n",
      "Epoch 12/500\n",
      "4/4 - 2s - loss: 0.4687 - accuracy: 0.4960 - 2s/epoch - 531ms/step\n",
      "Epoch 13/500\n",
      "4/4 - 2s - loss: 0.4647 - accuracy: 0.4936 - 2s/epoch - 500ms/step\n",
      "Epoch 14/500\n",
      "4/4 - 2s - loss: 0.4607 - accuracy: 0.4921 - 2s/epoch - 493ms/step\n",
      "Epoch 15/500\n",
      "4/4 - 2s - loss: 0.4566 - accuracy: 0.4911 - 2s/epoch - 492ms/step\n",
      "Epoch 16/500\n",
      "4/4 - 2s - loss: 0.4526 - accuracy: 0.4895 - 2s/epoch - 492ms/step\n",
      "Epoch 17/500\n",
      "4/4 - 2s - loss: 0.4486 - accuracy: 0.4891 - 2s/epoch - 497ms/step\n",
      "Epoch 18/500\n",
      "4/4 - 2s - loss: 0.4446 - accuracy: 0.4890 - 2s/epoch - 489ms/step\n",
      "Epoch 19/500\n",
      "4/4 - 2s - loss: 0.4406 - accuracy: 0.4897 - 2s/epoch - 495ms/step\n",
      "Epoch 20/500\n",
      "4/4 - 2s - loss: 0.4366 - accuracy: 0.4909 - 2s/epoch - 505ms/step\n",
      "Epoch 21/500\n",
      "4/4 - 2s - loss: 0.4326 - accuracy: 0.4916 - 2s/epoch - 493ms/step\n",
      "Epoch 22/500\n",
      "4/4 - 2s - loss: 0.4286 - accuracy: 0.4885 - 2s/epoch - 493ms/step\n",
      "Epoch 23/500\n",
      "4/4 - 2s - loss: 0.4246 - accuracy: 0.4994 - 2s/epoch - 514ms/step\n",
      "Epoch 24/500\n",
      "4/4 - 2s - loss: 0.4211 - accuracy: 0.5014 - 2s/epoch - 485ms/step\n",
      "Epoch 25/500\n",
      "4/4 - 2s - loss: 0.4312 - accuracy: 0.4388 - 2s/epoch - 489ms/step\n",
      "Epoch 26/500\n",
      "4/4 - 2s - loss: 0.4133 - accuracy: 0.5652 - 2s/epoch - 492ms/step\n",
      "Epoch 27/500\n",
      "4/4 - 2s - loss: 0.4089 - accuracy: 0.5523 - 2s/epoch - 494ms/step\n",
      "Epoch 28/500\n",
      "4/4 - 2s - loss: 0.4049 - accuracy: 0.5539 - 2s/epoch - 503ms/step\n",
      "Epoch 29/500\n",
      "4/4 - 2s - loss: 0.4008 - accuracy: 0.5370 - 2s/epoch - 502ms/step\n",
      "Epoch 30/500\n",
      "4/4 - 2s - loss: 0.3968 - accuracy: 0.5408 - 2s/epoch - 495ms/step\n",
      "Epoch 31/500\n",
      "4/4 - 2s - loss: 0.3927 - accuracy: 0.5428 - 2s/epoch - 503ms/step\n",
      "Epoch 32/500\n",
      "4/4 - 2s - loss: 0.3887 - accuracy: 0.5445 - 2s/epoch - 499ms/step\n",
      "Epoch 33/500\n",
      "4/4 - 2s - loss: 0.3847 - accuracy: 0.5467 - 2s/epoch - 492ms/step\n",
      "Epoch 34/500\n",
      "4/4 - 2s - loss: 0.3807 - accuracy: 0.5468 - 2s/epoch - 528ms/step\n",
      "Epoch 35/500\n",
      "4/4 - 2s - loss: 0.3767 - accuracy: 0.5446 - 2s/epoch - 491ms/step\n",
      "Epoch 36/500\n",
      "4/4 - 2s - loss: 0.3727 - accuracy: 0.5475 - 2s/epoch - 510ms/step\n",
      "Epoch 37/500\n",
      "4/4 - 2s - loss: 0.3688 - accuracy: 0.5301 - 2s/epoch - 497ms/step\n",
      "Epoch 38/500\n",
      "4/4 - 2s - loss: 0.5321 - accuracy: 0.4882 - 2s/epoch - 483ms/step\n",
      "Epoch 39/500\n",
      "4/4 - 2s - loss: 0.3671 - accuracy: 0.4855 - 2s/epoch - 492ms/step\n",
      "Epoch 40/500\n",
      "4/4 - 2s - loss: 0.3605 - accuracy: 0.4590 - 2s/epoch - 489ms/step\n",
      "Epoch 41/500\n",
      "4/4 - 2s - loss: 0.3560 - accuracy: 0.4557 - 2s/epoch - 486ms/step\n",
      "Epoch 42/500\n",
      "4/4 - 2s - loss: 0.3521 - accuracy: 0.5011 - 2s/epoch - 491ms/step\n",
      "Epoch 43/500\n",
      "4/4 - 2s - loss: 0.3473 - accuracy: 0.4930 - 2s/epoch - 496ms/step\n",
      "Epoch 44/500\n",
      "4/4 - 2s - loss: 0.3429 - accuracy: 0.4846 - 2s/epoch - 521ms/step\n",
      "Epoch 45/500\n",
      "4/4 - 2s - loss: 0.3387 - accuracy: 0.4736 - 2s/epoch - 529ms/step\n",
      "Epoch 46/500\n",
      "4/4 - 2s - loss: 0.3346 - accuracy: 0.4630 - 2s/epoch - 496ms/step\n",
      "Epoch 47/500\n",
      "4/4 - 2s - loss: 0.3305 - accuracy: 0.4693 - 2s/epoch - 491ms/step\n",
      "Epoch 48/500\n",
      "4/4 - 2s - loss: 0.3307 - accuracy: 0.4712 - 2s/epoch - 483ms/step\n",
      "Epoch 49/500\n",
      "4/4 - 2s - loss: 0.4063 - accuracy: 0.5241 - 2s/epoch - 492ms/step\n",
      "Epoch 50/500\n",
      "4/4 - 2s - loss: 0.3841 - accuracy: 0.4981 - 2s/epoch - 485ms/step\n",
      "Epoch 51/500\n",
      "4/4 - 2s - loss: 0.3193 - accuracy: 0.5163 - 2s/epoch - 490ms/step\n",
      "Epoch 52/500\n",
      "4/4 - 2s - loss: 0.3143 - accuracy: 0.5051 - 2s/epoch - 498ms/step\n",
      "Epoch 53/500\n",
      "4/4 - 2s - loss: 0.3113 - accuracy: 0.4847 - 2s/epoch - 497ms/step\n",
      "Epoch 54/500\n",
      "4/4 - 2s - loss: 0.3082 - accuracy: 0.5225 - 2s/epoch - 485ms/step\n",
      "Epoch 55/500\n",
      "4/4 - 2s - loss: 0.3020 - accuracy: 0.5221 - 2s/epoch - 488ms/step\n",
      "Epoch 56/500\n",
      "4/4 - 2s - loss: 0.2975 - accuracy: 0.4997 - 2s/epoch - 523ms/step\n",
      "Epoch 57/500\n",
      "4/4 - 2s - loss: 0.2932 - accuracy: 0.4911 - 2s/epoch - 489ms/step\n",
      "Epoch 58/500\n",
      "4/4 - 2s - loss: 0.2892 - accuracy: 0.4953 - 2s/epoch - 485ms/step\n",
      "Epoch 59/500\n",
      "4/4 - 2s - loss: 0.2882 - accuracy: 0.5016 - 2s/epoch - 505ms/step\n",
      "Epoch 60/500\n",
      "4/4 - 2s - loss: 0.2833 - accuracy: 0.5116 - 2s/epoch - 501ms/step\n",
      "Epoch 61/500\n",
      "4/4 - 2s - loss: 0.2924 - accuracy: 0.5406 - 2s/epoch - 498ms/step\n",
      "Epoch 62/500\n",
      "4/4 - 2s - loss: 0.2772 - accuracy: 0.5267 - 2s/epoch - 487ms/step\n",
      "Epoch 63/500\n",
      "4/4 - 2s - loss: 0.2719 - accuracy: 0.5473 - 2s/epoch - 491ms/step\n",
      "Epoch 64/500\n",
      "4/4 - 2s - loss: 0.2666 - accuracy: 0.5067 - 2s/epoch - 495ms/step\n",
      "Epoch 65/500\n",
      "4/4 - 2s - loss: 0.2622 - accuracy: 0.5129 - 2s/epoch - 496ms/step\n",
      "Epoch 66/500\n",
      "4/4 - 2s - loss: 0.2579 - accuracy: 0.5098 - 2s/epoch - 487ms/step\n",
      "Epoch 67/500\n",
      "4/4 - 2s - loss: 0.2536 - accuracy: 0.4925 - 2s/epoch - 501ms/step\n",
      "Epoch 68/500\n",
      "4/4 - 2s - loss: 0.2495 - accuracy: 0.4674 - 2s/epoch - 495ms/step\n",
      "Epoch 69/500\n",
      "4/4 - 2s - loss: 0.2484 - accuracy: 0.5531 - 2s/epoch - 494ms/step\n",
      "Epoch 70/500\n",
      "4/4 - 2s - loss: 0.2829 - accuracy: 0.4998 - 2s/epoch - 492ms/step\n",
      "Epoch 71/500\n",
      "4/4 - 2s - loss: 0.2500 - accuracy: 0.4574 - 2s/epoch - 499ms/step\n",
      "Epoch 72/500\n",
      "4/4 - 2s - loss: 0.2370 - accuracy: 0.5736 - 2s/epoch - 499ms/step\n",
      "Epoch 73/500\n",
      "4/4 - 2s - loss: 0.2358 - accuracy: 0.5669 - 2s/epoch - 505ms/step\n",
      "Epoch 74/500\n",
      "4/4 - 2s - loss: 0.2273 - accuracy: 0.5567 - 2s/epoch - 492ms/step\n",
      "Epoch 75/500\n",
      "4/4 - 2s - loss: 0.2227 - accuracy: 0.5432 - 2s/epoch - 487ms/step\n",
      "Epoch 76/500\n",
      "4/4 - 2s - loss: 0.2186 - accuracy: 0.4790 - 2s/epoch - 500ms/step\n",
      "Epoch 77/500\n",
      "4/4 - 2s - loss: 0.2143 - accuracy: 0.4920 - 2s/epoch - 519ms/step\n",
      "Epoch 78/500\n",
      "4/4 - 2s - loss: 0.2111 - accuracy: 0.5095 - 2s/epoch - 498ms/step\n",
      "Epoch 79/500\n",
      "4/4 - 2s - loss: 0.2072 - accuracy: 0.5188 - 2s/epoch - 491ms/step\n",
      "Epoch 80/500\n",
      "4/4 - 2s - loss: 0.2034 - accuracy: 0.4875 - 2s/epoch - 533ms/step\n",
      "Epoch 81/500\n",
      "4/4 - 2s - loss: 0.1994 - accuracy: 0.5115 - 2s/epoch - 504ms/step\n",
      "Epoch 82/500\n",
      "4/4 - 2s - loss: 0.1949 - accuracy: 0.4714 - 2s/epoch - 492ms/step\n",
      "Epoch 83/500\n",
      "4/4 - 2s - loss: 0.2322 - accuracy: 0.4360 - 2s/epoch - 488ms/step\n",
      "Epoch 84/500\n",
      "4/4 - 2s - loss: 0.2861 - accuracy: 0.6064 - 2s/epoch - 497ms/step\n",
      "Epoch 85/500\n",
      "4/4 - 2s - loss: 0.1949 - accuracy: 0.5983 - 2s/epoch - 498ms/step\n",
      "Epoch 86/500\n",
      "4/4 - 2s - loss: 0.1869 - accuracy: 0.6042 - 2s/epoch - 498ms/step\n",
      "Epoch 87/500\n",
      "4/4 - 2s - loss: 0.1809 - accuracy: 0.6041 - 2s/epoch - 485ms/step\n",
      "Epoch 88/500\n",
      "4/4 - 2s - loss: 0.2589 - accuracy: 0.5682 - 2s/epoch - 528ms/step\n",
      "Epoch 89/500\n",
      "4/4 - 2s - loss: 0.1964 - accuracy: 0.5925 - 2s/epoch - 496ms/step\n",
      "Epoch 90/500\n",
      "4/4 - 2s - loss: 0.1839 - accuracy: 0.6003 - 2s/epoch - 488ms/step\n",
      "Epoch 91/500\n",
      "4/4 - 2s - loss: 0.1741 - accuracy: 0.5860 - 2s/epoch - 491ms/step\n",
      "Epoch 92/500\n",
      "4/4 - 2s - loss: 0.1659 - accuracy: 0.5839 - 2s/epoch - 531ms/step\n",
      "Epoch 93/500\n",
      "4/4 - 2s - loss: 0.1599 - accuracy: 0.5837 - 2s/epoch - 528ms/step\n",
      "Epoch 94/500\n",
      "4/4 - 2s - loss: 0.1549 - accuracy: 0.5822 - 2s/epoch - 494ms/step\n",
      "Epoch 95/500\n",
      "4/4 - 2s - loss: 0.1515 - accuracy: 0.5890 - 2s/epoch - 509ms/step\n",
      "Epoch 96/500\n",
      "4/4 - 2s - loss: 0.1519 - accuracy: 0.5896 - 2s/epoch - 512ms/step\n",
      "Epoch 97/500\n",
      "4/4 - 2s - loss: 0.1486 - accuracy: 0.5479 - 2s/epoch - 564ms/step\n",
      "Epoch 98/500\n",
      "4/4 - 2s - loss: 0.1413 - accuracy: 0.5498 - 2s/epoch - 527ms/step\n",
      "Epoch 99/500\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(train_data, batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 2)  # add validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'Singular_Model'\n",
    "#model2.save(filename)\n",
    "#saved_model = load_model(\"Singular Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo(image):\n",
    "    prep_color, prep_bw = prep_img(image_pathway)\n",
    "    test_predict = model.predict(prep_bw)\n",
    "    #print(test_predict)\n",
    "    print('test_range_A:', np.min(test_predict[:,:,0]), np.max(test_predict[:,:,0]))\n",
    "    print('test_range_B:', np.min(test_predict[:,:,1]), np.max(test_predict[:,:,1]))\n",
    "    \n",
    "    test_predict_reshaped= test_predict[0,:,:,:]\n",
    "    #test_predict_reshaped = test_predict_reshaped*128\n",
    "    test_predict_reshaped = (test_predict_reshaped*256)-128\n",
    "    #print(test_predict_reshaped)\n",
    "    BW_reshaped  = prep_bw[0,:,:,:]\n",
    "    \n",
    "    print('test_reshaped_A:', np.min(test_predict_reshaped[:,:,0]), np.max(test_predict_reshaped[:,:,0]))\n",
    "    print(len(np.unique(test_predict_reshaped[:,:,0]))) #, return_counts=True\n",
    "    print('test_reshaped_B:', np.min(test_predict_reshaped[:,:,1]), np.max(test_predict_reshaped[:,:,1]))\n",
    "    test_merged_LAB = np.dstack((BW_reshaped, test_predict_reshaped))\n",
    "    test_merged_rgb = color.lab2rgb(test_merged_LAB)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(test_merged_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('C:/Study/Semester2/Machine Learning/Github/bin_centers.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_photo(image_pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "596c776c-1e6c-4691-a2af-2124b46c197d",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
