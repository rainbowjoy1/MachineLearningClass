{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    #CHANGE THIS TO YOUR TRAIN FOLDER\n",
    "    image_folder = 'C:/Study/Semester2/Machine Learning/ML_images/Training'\n",
    "    \n",
    "    TOO_SMALL = 200000\n",
    "    SMALL = 500000\n",
    "    MEDIUM = 1200000\n",
    "    LARGE = 2000000\n",
    "    \n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    counter = 1\n",
    "    \n",
    "    img_height = len(X) #Usually 128. this might change depending on Annaya & Danielle's input \n",
    "    img_width = len(X[0]) #Usually 128. this might change depending on Annaya & Danielle's input \n",
    "    epochs = 1 #Start with 1, and increase to 10, 100, 500, 1000 and 3000. 11 is the recommended number of runs through the training dataset. We will probably have to tune this. \n",
    "    #total_training = total number of training data len()\n",
    "    batch_size = 1 #test different ones from 1 to 2, 5, 10, etc\n",
    "    total_images = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "cell_id": "6ba26cc8e50f49b5b9b751f88421f6cc",
    "deepnote_cell_height": 459,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10265,
    "execution_start": 1652547014086,
    "source_hash": "25a5d69b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, UpSampling2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage import data, io,color\n",
    "from skimage.io import imread_collection #loads a collection of images\n",
    "from skimage.io import imread, imshow\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "cell_id": "5647a315e91e408b8dcdd46d33814f9e",
    "deepnote_cell_height": 477,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1652547146694,
    "source_hash": "8fbce57",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define sizes of images based on their resolution\n",
    "#Image shape and resolution\n",
    "\n",
    "#TODO make the IMG sizes make sense\n",
    "\n",
    "\n",
    "def shape(img):\n",
    "    \n",
    "    size = 0\n",
    "    resolution = img.shape[0] * img.shape[1]\n",
    "    if resolution <= 200000: #<= TOO_SMALL\n",
    "        size = 0\n",
    "    elif resolution <= 500000: #SMALL\n",
    "        size = 1\n",
    "    elif resolution <= 1200000: #MEDIUM\n",
    "        size = 2\n",
    "    elif resolution <= 2000000:#LARGE\n",
    "        size = 3\n",
    "    else:\n",
    "        size = 6\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(image_folder):\n",
    "    images = Path(image_folder).glob('*.jpg')\n",
    "\n",
    "    list_of_files = []\n",
    "    for image in images:\n",
    "        list_of_files.append(str(image))\n",
    "\n",
    "    df = pd. DataFrame(list_of_files, columns = [\"image_path\"])\n",
    "    df_length = len(df.index)\n",
    "    print(\"processing\", df_length, \"photos for training\")\n",
    "    \n",
    "    height= []\n",
    "    width = []\n",
    "    resolution = []\n",
    "    \n",
    "    for image_path in df['image_path']:\n",
    "        img = cv2.imread(str(image_path))\n",
    "        height.append(img.shape[0])\n",
    "        width.append(img.shape[1])\n",
    "        \n",
    "    df[\"height\"] = height\n",
    "    df[\"width\"] = width\n",
    "    df[\"resolution\"] = df[\"height\"] * df[\"width\"]\n",
    "    \n",
    "    df = df[df.resolution < 2000000] # < LARGE\n",
    "    \n",
    "    ll = df_length - len(df.index)\n",
    "    print(\"dropped \", ll, \"photos because they were too large to process\")\n",
    "    \n",
    "    df = df[df.resolution > 200000] #> TOO_SMALL\n",
    "    \n",
    "    ss = df_length - len(df.index) - ll\n",
    "    print(\"dropped \", ss, \"photos because they were too small to process\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "cell_id": "284e594fca804177bb7508f8e816bfa2",
    "deepnote_cell_height": 387,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652547146695,
    "source_hash": "4c92008b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PCA rgb then pass to prep_img as an rgb image\n",
    "\n",
    "def pca_rgb(imgRGB, size): #imgLAB imgRGB is fresh images from col\n",
    "    b,g,r = cv2.split(imgRGB)\n",
    "    list_channels = [b,g,r]\n",
    "    \n",
    "    #check size first\n",
    "    if size ==1:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 100)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        rgb_compressed = cv2.merge(inverted_img)    \n",
    "\n",
    "    elif size == 2:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 300)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        rgb_compressed = cv2.merge(inverted_img)\n",
    "    else:\n",
    "        inverted_img = []\n",
    "        for c in list_channels:            \n",
    "            pca = PCA(n_components = 500)\n",
    "            transformed = pca.fit_transform(c/255) #r_scaled\n",
    "            inverted = pca.inverse_transform(transformed)\n",
    "            inverted_img.append(inverted)\n",
    "        \n",
    "        tuple(inverted_img)\n",
    "        rgb_compressed = cv2.merge(inverted_img)\n",
    "\n",
    "    return rgb_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image padding\n",
    "\n",
    "def pad(image):\n",
    "    \n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "\n",
    "    WHITE = [255,255,255]\n",
    "    MAX = 1888\n",
    "    \n",
    "    if height % 2 == 0:\n",
    "        top = int((MAX - height)/ 2)\n",
    "        bottom = int((MAX - height)/ 2)\n",
    "    else: \n",
    "        top = int((MAX - (height-1))/ 2)\n",
    "        bottom = int(((MAX - (height-1))/ 2)-1)\n",
    "\n",
    "    if width % 2 == 0:\n",
    "        left = int((MAX - width)/ 2)\n",
    "        right = int((MAX - width)/ 2)\n",
    "    else: \n",
    "        left = int((MAX - (width-1))/ 2)\n",
    "        right = int(((MAX - (width-1))/2)-1)\n",
    "    \n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right,\n",
    "    cv2.BORDER_CONSTANT,value=WHITE)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "cell_id": "6be9a9aec0ee44dab9bb55d664de627f",
    "deepnote_cell_height": 693,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652547146783,
    "source_hash": "c4f5421",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function to preprocess image: by check image category, compress, separate color channels, and pad\n",
    "# gives out padded images in 2 groups,which are padded Lambda channel (Black&white) and AB channel(colors)\n",
    "\n",
    "#We can decide to take file path or im collection  or list\n",
    "#note: we are most familiar with controlling list\n",
    "n = 1\n",
    "\n",
    "def prep_img(img_path):\n",
    "    imgRGB = cv2.imread(img_path)\n",
    "\n",
    "    #check the image category by 'shape' function\n",
    "    image_size_category = shape(imgRGB)\n",
    "    #print(\"image size determined!\")\n",
    "\n",
    "    if image_size_category <1:\n",
    "        raise Exception(\"Your image is too small to be used\")\n",
    "    elif image_size_category >4:\n",
    "        raise Exception(\"Your image is too big to be used\")\n",
    "\n",
    "    #print(\"image PCA started!\")\n",
    "    image_pca = pca_rgb(imgRGB, image_size_category) \n",
    " \n",
    "    #image_pca.astype(\"float32\")/ 255 \n",
    "    image_pca = np.float32(1.0/255*image_pca)\n",
    "    imgLAB = cv2.cvtColor(image_pca, cv2.COLOR_RGB2Lab)\n",
    "    \n",
    "    #print(\"image converted to LAB\")\n",
    "    padded_image = pad(imgLAB)\n",
    "    \n",
    "    #separate L and AB color channels\n",
    "    Y = padded_image[:, :,1:]\n",
    "    X = padded_image[:, :, 0]\n",
    "    \n",
    "    Y = Y / 128\n",
    "    #this is in sample code and idk why\n",
    "    \n",
    "    X = X.reshape(1, 1888, 1888, 1)\n",
    "    Y = Y.reshape(1, 1888, 1888, 2)\n",
    "    \n",
    "    #for index, item in enumerate(iterable):\n",
    "    #print(index, item)\n",
    "    \n",
    "    print(\"image processed\")\n",
    "\n",
    "    return Y, X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def prep_dataframe(df):\n",
    "    #for count, index, row in enumerate(df.iterrows()): \n",
    "        #Y, X = prep_img(row[\"image_path\"])\n",
    "        #Y_train.append(Y)\n",
    "        #X_train.append(X)\n",
    "        #print(\"Processed\", count, \"images for upload\")\n",
    "#enumerate function is in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    " image_folder = 'C:/Study/Semester2/Machine Learning/ML_images/Training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    #TODO make it work with init() \n",
    "    df = create_dataframe(image_folder)\n",
    "    #call prep_img() which return Y,X\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    for i in df['image_path']:\n",
    "        Y_prep, X_prep = prep_img(i) # another choice is to name a vari\n",
    "    #append.X (ref with path in the df) to a list of  X images array\n",
    "    #append.Y to another list of Y images array          \n",
    "        X_list.append(X_prep)\n",
    "        Y_list.append(Y_prep)\n",
    "    #train test val split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_list, Y_list, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "    print(len(X_train), len(Y_train))\n",
    "    print(len(X_val), len(Y_val))\n",
    "    print(len(X_test), len(Y_test))\n",
    "  \n",
    "    print(\"Processed\", \"images for upload\")\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the code below to process ALL photos and feed them to the model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def preprocessing ():\n",
    "    #init\n",
    "    #df = create_dataframe(image_folder)\n",
    "    #prep_dataframe(df)\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 151 photos for training\n",
      "dropped  10 photos because they were too large to process\n",
      "dropped  5 photos because they were too small to process\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "image processed\n",
      "81 81\n",
      "27 27\n",
      "28 28\n",
      "Processed images for upload\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(): \n",
    "    model1 = Sequential([\n",
    "        #encoder\n",
    "        Conv2D(128, 3, strides = 2, padding = \"same\", activation = \"relu\", input_shape= (1888, 1888, 1)), #plug in the shape value #(img_height, img_width, 1)  \n",
    "        Conv2D(256, 3, strides = 2, padding = \"same\", activation = \"relu\"), \n",
    "        Conv2D(256, 3, strides = 2, padding = \"same\", activation = \"relu\"), \n",
    "\n",
    "        #decoder\n",
    "        Conv2D(256, 3, padding = \"same\", activation = \"relu\"), \n",
    "        MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),\n",
    "        UpSampling2D(2),\n",
    "        Conv2D(128, 3, padding = \"same\", activation = \"relu\"), \n",
    "        UpSampling2D(2), \n",
    "        Conv2D(2, 3, padding = \"same\", activation = \"tanh\"),\n",
    "        UpSampling2D(2),\n",
    "        #this is just trying to add something, see if the error changes\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dense(4, activation='softmax'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model1.compile(optimizer =\"adam\", loss = \"mse\", metrics = [\"accuracy\"])\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 944, 944, 128)     1280      \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 472, 472, 256)     295168    \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 236, 236, 256)     590080    \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 236, 236, 256)     590080    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 236, 236, 256)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 472, 472, 256)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 472, 472, 128)     295040    \n",
      "                                                                 \n",
      " up_sampling2d_10 (UpSamplin  (None, 944, 944, 128)    0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 944, 944, 2)       2306      \n",
      "                                                                 \n",
      " up_sampling2d_11 (UpSamplin  (None, 1888, 1888, 2)    0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1888, 1888, 32)    96        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1888, 1888, 4)     132       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1888, 1888, 2)     10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,774,192\n",
      "Trainable params: 1,774,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = mode()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following copy from EmiilWalner https://emilwallner.medium.com/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "batch_size = 3\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(X_train, batch_size=batch_size):\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)\n",
    "#We use the images from our folder, Xtrain, to generate images based on the settings above. \n",
    "#Then, we extract the black and white layer for the X_batch and the two colors for the two color layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-17be6156365e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_a_b_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/28 - 271s - loss: 52364.6016 - accuracy: 0.0156 - 271s/epoch - 10s/step\n",
      "Epoch 2/10\n",
      "28/28 - 240s - loss: 52364.5039 - accuracy: 0.0156 - 240s/epoch - 9s/step\n",
      "Epoch 3/10\n",
      "28/28 - 248s - loss: 52364.5039 - accuracy: 0.0156 - 248s/epoch - 9s/step\n",
      "Epoch 4/10\n",
      "28/28 - 254s - loss: 52364.5039 - accuracy: 0.0156 - 254s/epoch - 9s/step\n",
      "Epoch 5/10\n",
      "28/28 - 239s - loss: 52364.5039 - accuracy: 0.0156 - 239s/epoch - 9s/step\n",
      "Epoch 6/10\n",
      "28/28 - 253s - loss: 52364.4883 - accuracy: 0.0156 - 253s/epoch - 9s/step\n",
      "Epoch 7/10\n",
      "28/28 - 239s - loss: 52364.4688 - accuracy: 0.0156 - 239s/epoch - 9s/step\n",
      "Epoch 8/10\n",
      "28/28 - 220s - loss: 52364.4805 - accuracy: 0.0156 - 220s/epoch - 8s/step\n",
      "Epoch 9/10\n",
      "28/28 - 210s - loss: 52364.4883 - accuracy: 0.0156 - 210s/epoch - 7s/step\n",
      "Epoch 10/10\n",
      "28/28 - 209s - loss: 52364.4805 - accuracy: 0.0156 - 209s/epoch - 7s/step\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(train_data, batch_size = batch_size, epochs = epochs, verbose = 2)  # add validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(history, filename)\n",
    " \n",
    "\n",
    "#loaded_model = joblib.load(filename)\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)\n"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "db6bc5cd-12ac-4fa4-b95f-4604d6441ef4",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
